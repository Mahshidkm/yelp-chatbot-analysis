The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Device set to use cuda:0
Device set to use cuda:3
Device set to use cuda:1
Device set to use cuda:2
Device set to use cuda:1
Device set to use cuda:2
/home/mkad/mahshid_env/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
Device set to use cuda:0
/home/mkad/mahshid_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 112 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Processing reviews:   0%|          | 0/73 [00:00<?, ?it/s]Device set to use cuda:3
Processing reviews:   1%|▏         | 1/73 [00:10<12:33, 10.47s/it]Processing reviews:   3%|▎         | 2/73 [00:13<07:11,  6.08s/it]Processing reviews:   4%|▍         | 3/73 [00:16<05:35,  4.80s/it]Processing reviews:   5%|▌         | 4/73 [00:20<04:55,  4.28s/it]Processing reviews:   7%|▋         | 5/73 [00:23<04:35,  4.05s/it]Processing reviews:   8%|▊         | 6/73 [00:27<04:28,  4.01s/it]Processing reviews:  10%|▉         | 7/73 [00:31<04:28,  4.07s/it]Processing reviews:  11%|█         | 8/73 [00:35<04:09,  3.84s/it]Processing reviews:  12%|█▏        | 9/73 [00:38<03:51,  3.61s/it]Processing reviews:  14%|█▎        | 10/73 [00:42<03:48,  3.63s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Processing reviews:  15%|█▌        | 11/73 [00:44<03:26,  3.32s/it]Processing reviews:  16%|█▋        | 12/73 [00:48<03:31,  3.47s/it]Processing reviews:  18%|█▊        | 13/73 [00:52<03:43,  3.73s/it]Processing reviews:  19%|█▉        | 14/73 [00:55<03:26,  3.50s/it]Processing reviews:  21%|██        | 15/73 [01:00<03:45,  3.89s/it]Processing reviews:  22%|██▏       | 16/73 [01:04<03:37,  3.82s/it]Processing reviews:  23%|██▎       | 17/73 [01:08<03:36,  3.87s/it]Processing reviews:  25%|██▍       | 18/73 [01:12<03:34,  3.90s/it]Processing reviews:  26%|██▌       | 19/73 [01:16<03:43,  4.13s/it]Processing reviews:  27%|██▋       | 20/73 [01:20<03:37,  4.10s/it]Processing reviews:  29%|██▉       | 21/73 [01:25<03:32,  4.09s/it]Processing reviews:  30%|███       | 22/73 [01:30<03:50,  4.51s/it]Processing reviews:  32%|███▏      | 23/73 [01:34<03:42,  4.45s/it]Processing reviews:  33%|███▎      | 24/73 [01:38<03:31,  4.31s/it]Processing reviews:  34%|███▍      | 25/73 [01:43<03:27,  4.32s/it]Processing reviews:  36%|███▌      | 26/73 [01:47<03:25,  4.37s/it]Processing reviews:  37%|███▋      | 27/73 [01:52<03:33,  4.65s/it]Processing reviews:  38%|███▊      | 28/73 [01:56<03:12,  4.27s/it]Processing reviews:  40%|███▉      | 29/73 [02:00<03:03,  4.17s/it]Processing reviews:  41%|████      | 30/73 [02:03<02:52,  4.02s/it]Processing reviews:  42%|████▏     | 31/73 [02:07<02:45,  3.94s/it]Processing reviews:  44%|████▍     | 32/73 [02:10<02:30,  3.66s/it]Processing reviews:  45%|████▌     | 33/73 [02:14<02:25,  3.64s/it]Processing reviews:  47%|████▋     | 34/73 [02:19<02:42,  4.17s/it]Processing reviews:  48%|████▊     | 35/73 [02:23<02:38,  4.16s/it]Processing reviews:  49%|████▉     | 36/73 [02:29<02:51,  4.64s/it]Processing reviews:  51%|█████     | 37/73 [02:33<02:37,  4.37s/it]Processing reviews:  52%|█████▏    | 38/73 [02:37<02:25,  4.17s/it]Processing reviews:  53%|█████▎    | 39/73 [02:40<02:11,  3.87s/it]Processing reviews:  55%|█████▍    | 40/73 [02:44<02:06,  3.85s/it]Processing reviews:  56%|█████▌    | 41/73 [02:47<01:59,  3.74s/it]Processing reviews:  58%|█████▊    | 42/73 [02:51<02:00,  3.89s/it]Processing reviews:  59%|█████▉    | 43/73 [02:55<01:53,  3.79s/it]Processing reviews:  60%|██████    | 44/73 [02:59<01:56,  4.00s/it]Processing reviews:  62%|██████▏   | 45/73 [03:03<01:51,  3.99s/it]Processing reviews:  63%|██████▎   | 46/73 [03:07<01:44,  3.88s/it]Processing reviews:  64%|██████▍   | 47/73 [03:11<01:39,  3.83s/it]Processing reviews:  66%|██████▌   | 48/73 [03:16<01:44,  4.17s/it]Processing reviews:  67%|██████▋   | 49/73 [03:19<01:31,  3.83s/it]Processing reviews:  68%|██████▊   | 50/73 [03:22<01:28,  3.84s/it]Processing reviews:  70%|██████▉   | 51/73 [03:26<01:22,  3.76s/it]Processing reviews:  71%|███████   | 52/73 [03:30<01:19,  3.79s/it]Processing reviews:  73%|███████▎  | 53/73 [03:34<01:17,  3.87s/it]Processing reviews:  74%|███████▍  | 54/73 [03:38<01:12,  3.81s/it]Processing reviews:  75%|███████▌  | 55/73 [03:41<01:08,  3.81s/it]Processing reviews:  77%|███████▋  | 56/73 [03:45<01:03,  3.71s/it]Processing reviews:  78%|███████▊  | 57/73 [03:49<01:02,  3.93s/it]Processing reviews:  79%|███████▉  | 58/73 [03:53<00:58,  3.91s/it]Processing reviews:  81%|████████  | 59/73 [03:58<00:57,  4.14s/it]Processing reviews:  82%|████████▏ | 60/73 [04:02<00:55,  4.25s/it]Processing reviews:  84%|████████▎ | 61/73 [04:08<00:56,  4.71s/it]Processing reviews:  85%|████████▍ | 62/73 [04:12<00:49,  4.46s/it]Processing reviews:  86%|████████▋ | 63/73 [04:17<00:45,  4.55s/it]Processing reviews:  88%|████████▊ | 64/73 [04:21<00:39,  4.40s/it]Processing reviews:  89%|████████▉ | 65/73 [04:24<00:32,  4.10s/it]Processing reviews:  90%|█████████ | 66/73 [04:27<00:26,  3.79s/it]Processing reviews:  92%|█████████▏| 67/73 [04:31<00:22,  3.72s/it]Processing reviews:  93%|█████████▎| 68/73 [04:36<00:20,  4.17s/it]Processing reviews:  95%|█████████▍| 69/73 [04:40<00:15,  3.97s/it]Processing reviews:  96%|█████████▌| 70/73 [04:44<00:12,  4.24s/it]Processing reviews:  97%|█████████▋| 71/73 [04:48<00:08,  4.14s/it]Processing reviews:  99%|█████████▊| 72/73 [04:53<00:04,  4.21s/it]Processing reviews: 100%|██████████| 73/73 [04:57<00:00,  4.34s/it]Processing reviews: 100%|██████████| 73/73 [04:57<00:00,  4.08s/it]
[rank0]:[W415 17:43:52.051995469 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
